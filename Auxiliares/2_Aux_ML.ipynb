{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demanda de transporte\n",
    "\n",
    "## `Auxiliar: Redes neuronales artificiales`\n",
    "\n",
    "**Junio 2024**<br>\n",
    "**Gabriel Nova** <br>\n",
    "**G.N.Nova@tudelft.nl** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Instrucciones`\n",
    "\n",
    "**Las sesiones de laboratorio son:**<br>\n",
    "* Entornos de aprendizaje donde trabajas con Jupyter y puedes resolver dudas.<br>\n",
    "* No son calificadas y no tienen que ser entregadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Configuración del entorno de trabajo`\n",
    "\n",
    "**Opción 1: Entorno local**<br>\n",
    "Descomenta la siguiente celda si estás ejecutando este cuaderno en tu entorno local, para instalar todas las dependencias en tu versión de Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Opción 2: Google Colab**<br>\n",
    "Descomenta la siguiente celda si estás ejecutando este cuaderno en Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/TransportDemand/CI5144\n",
    "#!pip install -r CI5144/requirements_colab.txt\n",
    "#!mv \"/content/CI5144/Auxiliares/data\" /content/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Aplicación: Modelando elecciones de vecindarios` <br>\n",
    "En esta sesión de laboratorio, analizaremos el comportamiento de elección de vecindarios. Comprender las preferencias de las personas sobre las características de los vecindarios es crucial para los urbanistas cuando (re)desarrollan vecindarios o diseñan políticas para abordar, por ejemplo, la segregación residencial. Durante esta sesión de laboratorio, aplicará modelos de elección discreta para descubrir las preferencias de las personas sobre atributos como la distancia al centro de la ciudad y la proporción de extranjeros en su vecindario. Además, explorará si las preferencias interactúan con covariables como la edad, el género, la propiedad de la vivienda, la propiedad del automóvil y el nivel de urbanización. Mientras lo hace, probará varias especificaciones de utilidad e interpretará los resultados de los modelos de elección discreta.\n",
    "\n",
    "Para este estudio, utilizamos datos de un experimento de elección declarada, que se llevó a cabo entre 2017 y 2018 en cuatro ciudades europeas: Hanover, Mainz, Berna y Zurich.\n",
    "\n",
    "**`Objetivos de aprendizaje del auxiliar`**\n",
    "\n",
    "Después de completar el auxiliar, podrás:\n",
    "* Entrenar un Perceptrón Multicapa (MLP) en datos de elección.\n",
    "* Entrenar un modelo de elección híbrido, utilizando PyTorch.\n",
    "* Balance entre el el comportamiento esperado y el ajuste del modelo.\n",
    "* Reflexionar sobre las fortalezas y debilidades de los enfoques de modelado basados en datos y en teoría.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**`Este laboratorio consta de 4 partes`**\n",
    "\n",
    "**Parte 1**: Preparación de datos\n",
    "\n",
    "**Parte 2**: El modelo lineal-aditivo RUM-MNL\n",
    "- Ejercicio 1: \"Disposición a pagar por tiendas de comestibles\"\n",
    "\n",
    "**Parte 3**: El Perceptrón Multicapa\n",
    "- Ejercicio 2: \"Entrenamiento del modelo MLP\"\n",
    "- Ejercicio 3: \"Añadiendo las características socio-demográficas\"\n",
    "\n",
    "**Parte 4**: El modelo L-MNL\n",
    "- Ejercicio 4: \"Característica MLP\"\n",
    "- Ejercicio 5: \"Pronóstico utilizando el modelo L-MNL\"\n",
    "\n",
    "\n",
    "y comprende **`5`** ejercicios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Importar packages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Python packages and modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import getcwd\n",
    "from pathlib import Path\n",
    "\n",
    "# Biogeme\n",
    "import biogeme.database as db\n",
    "import biogeme.biogeme as bio\n",
    "from biogeme import models\n",
    "from biogeme.expressions import Beta, Variable, log\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Preparar su base de datos**\n",
    "Para preparar la base de datos, haremos lo siguiente:<br>\n",
    "    1.1 Cargar la base de datos<br>\n",
    "    1.2 Limpiar y preparar la base de datos<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `i. Configurar el espacio de trabajo y cargar la base de datos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el ruta a la base de datos\n",
    "data_path = Path(f'data/choice_data.dat')\n",
    "\n",
    "# Cargar datos de elección de modo en un pandas DataFrame\n",
    "df = pd.read_csv(data_path, sep='\\t')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Descripción de variables**<br>\n",
    "\n",
    "Por lo tanto, se recomienda revisar la descripción de la base de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable       | Descripción                                                    | Tipo/Niveles |\n",
    "|----------------|---------------------------------------------------------------|--------------|\n",
    "| `ID`           | ID del encuestado                                             | Entero       |\n",
    "| `TASK_ID`      | Id de la tarea de elección                                    | Entero       |\n",
    "| `STORES`       | Minutos caminando a los supermercados        | 2 Min., 5 Min., 10 Min., 15 Min. |\n",
    "| `TRANSPORT`    | Minutos caminando al transporte público           | 2 Min., 5 Min., 10 Min., 15 Min. |\n",
    "| `CITY`         | Distancia al centro de la ciudad en km                        | Menos de 1 km, 1 a 2 km, 3 a 4 km, más de 4 km |\n",
    "| `NOISE`        | Ruido del tráfico en la calle                                 | 1 = Ninguno, 2 = Poco, 3 = Medio, 4 = Alto |\n",
    "| `GREEN`        | Áreas verdes en la zona residencial                           | 1 = Ninguno, 2 = Pocas, 3 = Algunas, 4 = Muchas |\n",
    "| `FOREIGN`      | Proporción de extranjeros en la zona residencial              | 0.10, 0.20, 0.30, 0.40 |\n",
    "| `CHOICE`       | Indica la elección                                            | Entero       |\n",
    "| `RESPCITY`     | Indica la ciudad. 1 = Mainz, 2 = Hanover, 3 = Berna, 4 = Zurich | Categórico   |\n",
    "| `WOMAN`        | Indica 1 si es mujer y 0 de lo contrario                      | Categórico      |\n",
    "| `AGE`          | Edad en años                                                  | Entero       |\n",
    "| `ENVCONC`      | Preocupación ambiental de 1 a 5, siendo 5 el grado más alto de preocupación | Ordinal     |\n",
    "| `EDUYEARS`     | Número de años de educación                                   | Numérico     |\n",
    "| `RESPFOREIGN`  | 1 si el encuestado es extranjero, 0 de lo contrario           | Categórico      |\n",
    "| `HOMEOWNER`    | Indica 1 si el encuestado es propietario de vivienda y 0 de lo contrario | Categórico      |\n",
    "| `CAROWNER`     | Indica 1 si el encuestado es propietario de un auto y 0 de lo contrario | Categórico      |\n",
    "| `JOB`          | 1 si el encuestado está trabajando, 0 de lo contrario         | Categórico      |\n",
    "| `NONWESTERN`   | 1 si el encuestado es no occidental, 0 de lo contrario        | Categórico      |\n",
    "| `WESTERN`      | 1 si el encuestado es occidental, 0 de lo contrario           | Categórico      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Inspeccionar, limpiar y preparar los datos**<br>\n",
    "Antes de empezar a analizar los datos, asegúrate de que sabes qué conoces las características que posee.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tabla de arriba proporciona información importante sobre las variables de la base de datos. El promedio y la desviación estándar (std) de algunas variables son altas. Por ejemplo, los valores medios de `WOMAN`, `AGE`, `EDUYEARS` son bastante más altos de lo esperado según la descripción de las variables. Esto podría indicar problemas en el conjunto de datos, la presencia de valores atípicos o valores perdidos. En la tabla anterior, puede verse que el número `99999` no coincide con los valores habituales de variables como `WOMEN`, `AGE` y `others`.<br>\n",
    "\n",
    "Esto sugiere que el valor `99999` se ha utilizado para representar `valores perdidos` en el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Conjunto de datos para la estimación`**<br>\n",
    "\n",
    "Dado nuestro objetivo de investigar el impacto de la edad, el sexo, la propiedad de la vivienda, la propiedad de un auto y la urbanización, tenemos que trabajar con las filas de los datos con valores que faltan en estas columnas. Afortunadamente, la proporción de datos que faltan en estas columnas es relativamente pequeña. Por lo tanto, una estrategia razonable en este caso es simplemente eliminar las filas de datos con valores que faltan en estas columnas. Otra estrategia podría haber sido imputar los valores de la media. Sin embargo, esta estrategia también tiene sus desventajas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista de características relevantes\n",
    "id         = ['ID', 'TASK_ID' ]\n",
    "attributes =   ['STORES1', 'TRANSPORT1', 'CITY1', 'NOISE1', 'GREEN1', 'FOREIGN1', \n",
    "                'STORES2', 'TRANSPORT2', 'CITY2', 'NOISE2', 'GREEN2', 'FOREIGN2',\n",
    "                'STORES3', 'TRANSPORT3', 'CITY3', 'NOISE3', 'GREEN3', 'FOREIGN3']\n",
    "\n",
    "sociovars = ['AGE','WOMAN','HOMEOWNER','CAROWNER','RESPCITY', 'JOB', 'EDUYEARS', 'ENVCONC']\n",
    "\n",
    "# Crear una nueva instancia del dataframe, con los atributos, las variables socio-demográficas y la elección.\n",
    "dff = df[ id + attributes + sociovars + ['CHOICE']].copy()\n",
    "\n",
    "# Reemplazar 9999 por NaN\n",
    "dff = dff.replace(99999, np.nan)\n",
    "\n",
    "# Eliminar las filas con datos que faltan en las características relevantes\n",
    "dff = dff.dropna(subset=sociovars, how='any')\n",
    "\n",
    "dff.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff['ENVCONC']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recodificar las variables sociodemográficas categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un dataframe con las variables socio demográficas relevantes\n",
    "df_sociovars  = dff[sociovars].astype(int)\n",
    "\n",
    "# Recodificar 'AGE' \n",
    "df_sociovars.loc[(      df_sociovars['AGE'] < 25), 'AGE'] = 1\n",
    "df_sociovars.loc[(25 <= df_sociovars['AGE']     ) &\n",
    "                 (      df_sociovars['AGE'] < 60), 'AGE'] = 2\n",
    "df_sociovars.loc[(60 <= df_sociovars['AGE']     ), 'AGE'] = 3\n",
    "\n",
    "# Recodificar 'EDUYEARS'\n",
    "df_sociovars.loc[(      df_sociovars['EDUYEARS'] < 10), 'EDUYEARS'] = 1\n",
    "df_sociovars.loc[(10 <= df_sociovars['EDUYEARS']     ) & \n",
    "                 (      df_sociovars['EDUYEARS'] < 14), 'EDUYEARS'] = 2\n",
    "df_sociovars.loc[(14 <= df_sociovars['EDUYEARS']     ), 'EDUYEARS'] = 3\n",
    "\n",
    "# Recodificar\n",
    "df_sociovars.loc[(     df_sociovars['ENVCONC'] < 3), 'ENVCONC'] = 1\n",
    "df_sociovars.loc[(3 <= df_sociovars['ENVCONC']    ) & \n",
    "                 (     df_sociovars['ENVCONC'] < 4), 'ENVCONC'] = 2\n",
    "df_sociovars.loc[(4 <= df_sociovars['ENVCONC']    ), 'ENVCONC'] = 3\n",
    "\n",
    "#Convertir variables categóricas en variables dummies utilizando pd.get_dummies()\n",
    "df_sociovars = pd.get_dummies(data = df_sociovars, prefix = sociovars, prefix_sep='_', columns = sociovars, drop_first = True, dtype=int)\n",
    "\n",
    "# Crear una variable con los nombres de las variables sociodemográficas\n",
    "features_socio = df_sociovars.columns.tolist()\n",
    "\n",
    "df_sociovars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, cree el dataframe para el entrenamiento, que contiene el «ID», la «CHOICE», los atributos y las variables sociodemográficas codificados como dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenar las variables socio demográficas con las características relevantes de las alternativas\n",
    "dff = pd.concat([dff[['ID', 'CHOICE'] + attributes], df_sociovars], axis=1)\n",
    "\n",
    "print(dff.shape)\n",
    "dff.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ii. Dividir los datos en un conjunto de entrenamiento y otro de test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijar una semilla para la reproducibilidad\n",
    "np.random.seed(100)\n",
    "dff['draw']=float('nan')\n",
    "for i in dff['ID'].unique():\n",
    "    num_obs = len(dff.loc[dff['ID']==i,'draw'])\n",
    "    dff.loc[dff['ID']==i,'draw']= np.repeat(np.random.uniform(0,1,1),num_obs)\n",
    "\n",
    "# Poner el 80% de los datos en el conjunto de entrenamiento y el 20% en el conjunto de prueba\n",
    "df_train = dff.loc[dff['draw']< 0.8,:].copy()\n",
    "df_test  = dff.loc[dff['draw']>=0.8,:].copy()\n",
    "\n",
    "# Número de observaciones en los conjuntos de entrenamiento y prueba\n",
    "num_obs_train = len(df_train)\n",
    "num_obs_test  = len(df_test)\n",
    "\n",
    "print(f'Número de individuos en df_train y df_test: \\t{df_train.ID.nunique()}  {df_test.ID.nunique()} ')\n",
    "print(f'Número de observaciones en df_train y df_test: \\t{len(df_train)} {len(df_test)} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iii. Escalar las características`<br>\n",
    "Para entrenar RNA de forma eficiente, se recomienda escalar (es decir, normalizar) las características. Existen varias formas de escalar los datos. A menudo se utiliza un escalador de sk-learn llamado 'StandardScaler'. Este escalador normaliza la varianza y desplaza la ubicación de la distribución a cero, vea https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear X_train y X_test\n",
    "x_train = df_train[attributes + features_socio]\n",
    "x_test  = df_test [attributes + features_socio]\n",
    "\n",
    "# Inicializar el objeto scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajustar y transformar los datos de entrenamiento\n",
    "scaler = scaler.fit(dff[attributes + features_socio])\n",
    "\n",
    "# Aplicar el escalador ajustado a los conjuntos de datos\n",
    "x_train_scaled = pd.DataFrame(scaler.transform(df_train[attributes + features_socio]), columns=[attributes + features_socio])\n",
    "x_test_scaled =  pd.DataFrame(scaler.transform(df_test [attributes + features_socio]), columns=[attributes + features_socio]) \n",
    "\n",
    "print('Dimensión de x_train', x_train_scaled.shape)\n",
    "print('Dimensión de x_test', x_test_scaled.shape)\n",
    "\n",
    "# Crear valores de etiquetas \n",
    "# Y debe ser una matriz codificada como dummies\n",
    "y_train_dummy = pd.get_dummies(df_train['CHOICE']).values.astype(int)\n",
    "y_test_dummy = pd.get_dummies(df_test['CHOICE']).values.astype(int)\n",
    "\n",
    "y_train_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iv. Convertir a tensores`<br>\n",
    "Muchos paquetes de machine learning trabajan con los llamados tensores. Los tensores son estructuras de datos dedicadas a entrenar eficientemente redes neuronales. Por tanto, necesitamos convertir las características y el etiqueta en un tipo de datos tensores. En PyTorch, esto se hace con `torch.tensor()`.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tensores para el conjunto de entrenamiento\n",
    "# En este caso, usamos sólo las características de las alternativas 'features_alt'\n",
    "x_train_tensor = torch.tensor(x_train_scaled[attributes].values, dtype=torch.float)\n",
    "y_train_dummy_tensor = torch.tensor(y_train_dummy, dtype=torch.float)\n",
    "\n",
    "# Crear tensores para el conjunto de test\n",
    "x_test_tensor = torch.tensor(x_test_scaled[attributes].values, dtype=torch.float)\n",
    "y_test_dummy_tensor = torch.tensor(y_test_dummy, dtype=torch.float)\n",
    "\n",
    "print('Dimensión de x_train_tensor\\t', x_train_tensor.shape)\n",
    "print('Dimensión de x_test_tensor\\t', x_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2. Estimación de un modelo de benchmark RUM-MNL lineal aditivo` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`i. Estimación del modelo`<br>\n",
    "Primero estimamos nuestro modelo de benchmark RUM-MNL lineal-aditivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biodata_train = db.Database('neighbourhood_data_train', df_train)\n",
    "\n",
    "STORES1     = Variable('STORES1')\n",
    "TRANSPORT1  = Variable('TRANSPORT1')\n",
    "CITY1       = Variable('CITY1')\n",
    "NOISE1      = Variable('NOISE1')\n",
    "GREEN1      = Variable('GREEN1')\n",
    "FOREIGN1    = Variable('FOREIGN1')\n",
    "\n",
    "STORES2     = Variable('STORES2')\n",
    "TRANSPORT2  = Variable('TRANSPORT2')\n",
    "CITY2       = Variable('CITY2')\n",
    "NOISE2      = Variable('NOISE2')\n",
    "GREEN2      = Variable('GREEN2')\n",
    "FOREIGN2    = Variable('FOREIGN2')\n",
    "    \n",
    "STORES3     = Variable('STORES3')\n",
    "TRANSPORT3  = Variable('TRANSPORT3')\n",
    "CITY3       = Variable('CITY3')\n",
    "NOISE3      = Variable('NOISE3')\n",
    "GREEN3      = Variable('GREEN3')\n",
    "FOREIGN3    = Variable('FOREIGN3')\n",
    "\n",
    "CHOICE      = Variable('CHOICE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'Linear-additive RUM-MNL'\n",
    "\n",
    "B_stores    = Beta('B_stores'   , 0, None, None, 0)\n",
    "B_transport = Beta('B_transport', 0, None, None, 0)\n",
    "B_city      = Beta('B_city'     , 0, None, None, 0)\n",
    "B_noise     = Beta('B_noise'    , 0, None, None, 0)\n",
    "B_green     = Beta('B_green'    , 0, None, None, 0)\n",
    "B_foreign   = Beta('B_foreign'  , 0, None, None, 0)\n",
    "\n",
    "V1 = B_stores * STORES1 + B_transport * TRANSPORT1 + B_city * CITY1 + B_noise * NOISE1 + B_green * GREEN1 + B_foreign * FOREIGN1\n",
    "V2 = B_stores * STORES2 + B_transport * TRANSPORT2 + B_city * CITY2 + B_noise * NOISE2 + B_green * GREEN2 + B_foreign * FOREIGN2\n",
    "V3 = B_stores * STORES3 + B_transport * TRANSPORT3 + B_city * CITY3 + B_noise * NOISE3 + B_green * GREEN3 + B_foreign * FOREIGN3\n",
    "\n",
    "V  = {1: V1, 2: V2, 3: V3}    \n",
    "AV = {1: 1, 2: 1, 3: 1} \n",
    "\n",
    "prob = models.logit(V, AV, CHOICE)\n",
    "\n",
    "biogeme = bio.BIOGEME(biodata_train, log(prob))\n",
    "\n",
    "biogeme.generate_pickle = False\n",
    "biogeme.generate_html = False\n",
    "biogeme.saveIterations = False\n",
    "biogeme.modelName = model_name\n",
    "\n",
    "biogeme.calculateNullLoglikelihood(AV)\n",
    "results = biogeme.estimate()\n",
    "print(results.short_summary())\n",
    "\n",
    "beta_hat = results.getEstimatedParameters()\n",
    "beta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ii. Calcular la LL del modelo estimado en el conjunto de pruebas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el objeto Biogeme para la base de datos de prueba\n",
    "biodate_test = db.Database('Neighbourhood_data_test', df_test)\n",
    "\n",
    "# Simular la probabilidad de eleccion considerando el modelo MNL en los datos de prueba\n",
    "dict2simulate = {'prob_chosen':prob}\n",
    "\n",
    "# Crear el objeto de simulacion y calcular las probabilidades\n",
    "biosim = bio.BIOGEME(biodate_test,dict2simulate)\n",
    "simulated_probs = biosim.simulate(theBetaValues=results.getBetaValues())\n",
    "\n",
    "# Calcular Calculate la LL del MNL en la base de datos de prueba\n",
    "LL_MNL_test = np.log(simulated_probs['prob_chosen']).sum()\n",
    "print(f'LL MNL test: {LL_MNL_test:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``Ejercicio 1: Disposición a pagar por vivir mas cerca de los supermercados``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`A` Calcule la disposición a pagar para reducir el número de minutos caminando hasta las tiendas de comestibles en un minuto, expresado en términos de minutos caminando hasta el transporte público. Usa la fórmula::<br><br>\n",
    "$WTP = \\frac{\\beta_{stores}}{\\beta_{transport}}$<br><br>\n",
    "`B` Interprete este resultado. ¿Qué encuentran más importante las personas; minutos caminando hasta los supermercados o minutos caminando hasta el transporte público?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{green}{\\text{Añade tus respuestas aqui}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añade tu código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `3. El modelo MLP`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A El perceptrón multicapa (MLP) es una red neuronal ampliamente utilizada en el machine learning por su capacidad para aprender patrones complejos. Está compuesto por capas ocultas con neuronas y funciones de activación. <br>\n",
    "\n",
    "Por ejemplo, en la siguiente imagen, podemos ver una capa de entrada con 8 características, dos capas ocultas con 10 nodos ocultos cada una, y la capa de salida con 3 clases de salida:<br>\n",
    "\n",
    "<p align=\"center\\\">\n",
    "<img width=\"800\" src=\"Assets/MLP.png\">\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`i. Definición del modelo MLP`<br>\n",
    "\n",
    "Crearemos una red neuronal totalmente conectada con dos capas ocultas. Para ello, crearemos una nueva clase utilizando el módulo nn.Module de PyTorch. \n",
    "\n",
    "En este caso, tendremos una capa de entrada de tamaño `input_size`, dos capas ocultas de tamaños `hidden_size1` y `hidden_size2` respectivamente, y una capa de salida de tamaño `output_size`. \n",
    "\n",
    "Además, definiremos la función de propragación. La función de propragación toma un valor de entrada `x`, lo pasa a través de las capas de la red, al tiempo que aplica funciones de activación. \n",
    "\n",
    "El resultado son las utilidades de cada alternativa `V`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red neuronal totalmente conectada con dos capas ocultas\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x)) # función de activación tanh para la 1 capa\n",
    "        x = torch.tanh(self.fc2(x)) # función de activación tanh para la 2 capa\n",
    "        V = self.fc3(x)             # función de activación lineal para la capa de salida\n",
    "\n",
    "        return V                    # devuelve la «Utilidad» de cada alternativa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ii. Creación de DataLoaders`<br>\n",
    "A través de un objeto PyTorch Dataloader se maneja eficientemente el flujo de datos durante el entrenamiento y las evaluaciones. Este crea mini lotes, y se asegura de que las instancias de los datos se utilicen en cada época. <br>\n",
    "\n",
    "Crearemos un DataLoader para los conjuntos de datos de entrenamiento y prueba. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataLoader para el conjunto de entrenamientos\n",
    "dataset_train = TensorDataset(x_train_tensor, y_train_dummy_tensor)\n",
    "train_loader = DataLoader(dataset_train, batch_size=250, shuffle=True)\n",
    "\n",
    "# Crear un DataLoader para el conjunto de pruebas\n",
    "dataset_test = TensorDataset(x_test_tensor, y_test_dummy_tensor)\n",
    "test_loader = DataLoader(dataset_test, batch_size=len(x_test_tensor), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iii. Definir funciones`<br>\n",
    "Definiremos las siguientes funciones:\n",
    "1. Una función para evaluar el rendimiento del modelo durante el entrenamiento.\n",
    "2. Una función para visualizar el progreso del entrenamiento.\n",
    "3. Una función para examinar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, data_loader):\n",
    "    model.eval()  # Poner el modelo en modo evaluación\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función visualiza el progreso del entrenamiento. Llamamos a esta función una vez finalizado el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_loss_plot(train_losses, test_losses, num_obs_train, num_obs_test):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(np.array(train_losses)/num_obs_train, label='Training Loss')\n",
    "    ax.plot(np.array(test_losses)/num_obs_test,   label='Test Loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función nos permite inspeccionar la arquitectura del modelo creado. Enumera el número de pesos por capa y sus tamaños de entrada y salida. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_summary(model):\n",
    "    total_params = 0\n",
    "    print(f\"=== Model Summary ===\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            num_params = param.numel()\n",
    "            total_params += num_params\n",
    "            print(f\"Layer: {name:20}|\\t Weights: {num_params}\")\n",
    "\n",
    "    print(f\"\\nTotal trainable Weights: {total_params}\")\n",
    "\n",
    "    print(\"\\n=== Layers ===\")\n",
    "    for layer in model.children():\n",
    "        print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iv. Crear el objeto MLP`<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir las dimensiones del MLP\n",
    "input_size   = x_train_tensor.size()[1]  # Número de características de entrada\n",
    "hidden_size1 = 10                        # Número de nodos en la primera capa oculta\n",
    "hidden_size2 = 10                        # Número de nodos en la primera capa oculta\n",
    "output_size  = 3                         # Número de clases (determinado por el numero de alternativas)\n",
    "print(f' input_size = {input_size}, hidden_size1 = {hidden_size1}, hidden_size2 = {hidden_size2}, output_size = {output_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invocar el modelo MLP\n",
    "model = MLP(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "# Mostrar la arquitectura del modelo\n",
    "print_model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`v. Entrenar el modelo MLP`<br>\n",
    "Finalmente, ¡estamos listos para entrenar el modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEpoch = 500 # Establecer el número de epochs\n",
    "lr = 0.0001 # Establecer la tasa de aprendizaje\n",
    "status = 10 # Imprimir estado cada 'status' épocas\n",
    "patience = 5 # Número de epochs a esperar antes de parar preventivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_training = True\n",
    "if do_training == True:\n",
    "\n",
    "    # Definir el modelo, la función de pérdida y el optimizador\n",
    "    \n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=lr,  betas=(0.9, 0.999), eps=1e-07, weight_decay=0)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    # Definir listas para almacenar los valores de loss \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    # Inicializar el contador de parada anticipada \n",
    "    counter = 0\n",
    "    best_loss = np.inf  # Establece la pérdida inicial a infinito\n",
    "\n",
    "    # Bucle de entrenamiento\n",
    "    for epoch in range(nEpoch):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # Poner a cero los gradientes\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)   \n",
    "\n",
    "            # Calcular la perdida\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Backward pass y optimizacion\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Actualizar la pérdida\n",
    "            running_loss += loss.item() # Log-verosimilitud en el conjunto de entrenamiento\n",
    "\n",
    "        # Evaluar en el conjunto de prueba\n",
    "        test_loss = evaluate(model, loss_fn, test_loader) # Log-verosimilitud en el conjunto de prueba\n",
    "\n",
    "        \n",
    "        if epoch == 0 or (epoch + 1) % status == 0:\n",
    "            print(f'Epoch [{epoch+1:5.0f}/{nEpoch}], Train Loss: {running_loss:0.3f}, Test Loss: {test_loss:0.3f}')\n",
    "\n",
    "        # Almacenar pérdidas\n",
    "        train_losses.append(running_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        # Implementar la parada anticipada\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Parada anticipada en la epch {epoch+1}')\n",
    "                break\n",
    "            \n",
    "    print(f'\\nEntrenamiento finalizado. \\tPérdida de entrenamiento: {running_loss:0.3f}, Pérdida de test: {test_loss:0.3f}')\n",
    "    show_loss_plot(train_losses, test_losses,num_obs_train, num_obs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Ejercicio 2: Entrenamiento del modelo MLP`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`A` Compare el rendimiento en el conjunto de prueba del MLP con el MNL de referencia. ¿Ha mejorado (mucho)? ¿Qué nos dice esto? <br>\n",
    "`B` Reentrena el MLP utilizando las siguientes arquitecturas: {hidden_size1,hidden_size2} = {1,1}, {3,3}, {1,20}, {20,1}, {20,3}, {20,20}<br>\n",
    "`C` ¿El aumento del número de neuronas conduce a un mejor rendimiento de generalización (es decir, mayor LL_test)? ¿Qué ocurre?\n",
    "`D` Explique por qué {1,1} y {20,1} y {1,20} conducen a un rendimiento pobre. <br>\n",
    "`E` Reentrene su modelo con una tasa de aprendizaje menor y mayor: lr = 0,01 y lr = 0,00001. Use {hidden_size1,hidden_size2} = {5,5}. Explique lo que ocurre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{green}{\\text{Añada aquí sus respuestas}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Añada su código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Ejercicio 3: Añadir las características sociodemográficas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la sección 1, en `iv. Convertir en tensores` decidimos utilizar sólo las características de las alternativas como entradas (es decir, `features_alt`), mientras que en la sesión de laboratorio 1 vimos que al menos el género (`WOMAN`) y la ciudad de residencia (`RESPCITY`) tienen cierto poder explicativo.<br>\n",
    "\n",
    "`A` Modifica el código en esta celda para que también se utilicen estas características socio demográficas. Después, vuelva a entrenar el MLP.\n",
    "`B` Quizás en contra de sus expectativas, el rendimiento del modelo no aumenta mucho. \n",
    "¿Qué nos dice esto sobre: \n",
    "1. el poder explicativo de las características sociodemográficas para la elección de la zona residencial, y \n",
    "2. la capacidad de los modelos MLP para aprender efectos de interacción sutiles? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{green}{\\text{Añada aquí sus respuestas}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Añada su código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `4. El modelo L-MNL `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, construiremos y entrenaremos un modelo híbrido, concretamente el modelo L-MNL. Este modelo combina una parte interpretable siguiendo los supuestos del modelo MNL y una red neuronal que procesa el resto de variables que no queremos interpretar. Más concretamente, queremos la **flexibilidad** del MLP pero aún así queremos obtener una **estimación de la disposición a pagar por reducir el tiempo de caminata a los supermercados, expresada en términos del tiempo al transporte público**.\n",
    "\n",
    "La figura siguiente muestra conceptualmente este modelo:<br>\n",
    "\n",
    "<p align=\"center\">\n",
    "<img width=\"600\" src=\"assets/hybrid_model.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`i. Definir el modelo L-MNL`<br>\n",
    "Para crear el modelo L-MNL,creamos una nueva clase utilizando nn.Module de PyTorch. <br>\n",
    "* La parte MNL se implementa con capas lineales separadas para las características («transport» y «foreign»), permitiendo calcular la utilidad para cada alternativa. <br>\n",
    "* La parte MLP consiste en dos capas ocultas (linear1 y linear2) con un número dado de neuronas.<br>\n",
    "* La función forward toma `x_mnl` y `x_mlp` como entradas. x_mlp se pasa a través de las capas de la red (`V_MLP`), mientras que x_mnl se utiliza para calcular las utilidades de forma lineal aditiva (`V_MNL`). \n",
    "* Finalmente, se suman las utilidades de ambas partes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMNL(nn.Module):\n",
    "    def __init__(self, input_size_mlp, hidden_size1, hidden_size2,output_size):\n",
    "        super(LMNL,self).__init__()\n",
    "\n",
    "        # Crear betas para la parte MNL\n",
    "        self.B_transport = nn.Linear(1, 1, bias=False )\n",
    "        self.B_stores   = nn.Linear(1, 1, bias=False )\n",
    "                                 \n",
    "        # Crear las capas ocultas para la parte MLP \n",
    "        self.linear1 = nn.Linear(input_size_mlp, hidden_size1, bias=False)\n",
    "        self.linear2 = nn.Linear(hidden_size1,   hidden_size2, bias=False) \n",
    "        self.linear3 = nn.Linear(hidden_size2,   output_size,  bias=False) \n",
    "\n",
    "    def forward(self, X_MNL, X_MLP):\n",
    "        \n",
    "        # Funcion de utilidad para la parte MNL \n",
    "        V_A = self.B_transport(X_MNL[:,0].unsqueeze(1))  + self.B_stores(X_MNL[:,1].unsqueeze(1))\n",
    "        V_B = self.B_transport(X_MNL[:,2].unsqueeze(1))  + self.B_stores(X_MNL[:,3].unsqueeze(1))\n",
    "        V_C = self.B_transport(X_MNL[:,4].unsqueeze(1))  + self.B_stores(X_MNL[:,5].unsqueeze(1))\n",
    "                     \n",
    "        # Concatenating tensors to maintain the output dimension\n",
    "        V_MNL = torch.cat((V_A, V_B, V_C), dim=1)\n",
    "\n",
    "        # La parte MLP\n",
    "        X_MLP = torch.tanh(self.linear1(X_MLP)) \n",
    "        X_MLP = torch.tanh(self.linear2(X_MLP)) \n",
    "        V_MLP = self.linear3(X_MLP)             \n",
    "\n",
    "        V = V_MNL + V_MLP\n",
    "        return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ii. Preparar los datos para entrenar el modelo L-MNL`<br>\n",
    "\n",
    "Necesitamos dividir las características que van en las partes MLP y MNL. Porque queremos calcular la WTP de los supermercados sobre el transporte. Por lo tanto, las características relacionadas con el transporte y las tiendas van a la parte MNL, mientras que las demás características, incluidas las sociodemográficas, van a la parte MLP.\n",
    "\n",
    " **Importante**, las características MNL NO deben escalarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mnl = ['TRANSPORT1', 'STORES1','TRANSPORT2', 'STORES2','TRANSPORT3', 'STORES3']\n",
    "x_mnl_train = x_train[features_mnl]\n",
    "x_mnl_test  = x_test[features_mnl]\n",
    "\n",
    "print('Dimesión de x_mnl_train\\t', x_mnl_train.shape)\n",
    "print('Dimesión de x_mnl_test\\t', x_mnl_test.shape)\n",
    "\n",
    "features_mlp_alt = ['FOREIGN1', 'CITY1', 'NOISE1', 'GREEN1', \n",
    "                    'FOREIGN2', 'CITY2', 'NOISE2', 'GREEN2', \n",
    "                    'FOREIGN3', 'CITY3', 'NOISE3', 'GREEN3']\n",
    "\n",
    "\n",
    "features_socio = ['RESPCITY_2','RESPCITY_3','RESPCITY_4']\n",
    "x_mlp_train = x_train_scaled[features_mlp_alt + features_socio]\n",
    "x_mlp_test  = x_test_scaled[features_mlp_alt + features_socio]\n",
    "\n",
    "print('\\nDimesión de x_mlp_train\\t', x_mlp_train.shape)\n",
    "print('Dimesión de x_mlp_test\\t', x_mlp_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir datos en tensores PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento\n",
    "x_mnl_train_tensor = torch.tensor(x_mnl_train.values, dtype=torch.float)\n",
    "x_mlp_train_tensor = torch.tensor(x_mlp_train.values, dtype=torch.float)\n",
    "y_train_dummy_tensor = torch.tensor(y_train_dummy,    dtype=torch.float)\n",
    "\n",
    "# test\n",
    "x_mnl_test_tensor = torch.tensor(x_mnl_test.values, dtype=torch.float)\n",
    "x_mlp_test_tensor = torch.tensor(x_mlp_test.values, dtype=torch.float)\n",
    "y_test_dummy_tensor = torch.tensor(y_test_dummy,    dtype=torch.float)\n",
    "\n",
    "print('Tamaño del tensor X_mnl_train = ', x_mnl_train_tensor.size())\n",
    "print('Tamaño del tensor X_mlp_train = ', x_mlp_train_tensor.size())\n",
    "print('\\nTamaño del tensorX_mnl_test  = ', x_mnl_test_tensor.size())\n",
    "print('Tamaño del tensor X_mlp_test  = ', x_mlp_test_tensor.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un DataLoader para el conjunto de entrenamiento\n",
    "dataset_train_lmnl = TensorDataset(x_mnl_train_tensor, x_mlp_train_tensor, y_train_dummy_tensor)\n",
    "train_loader_lmnl = DataLoader(dataset_train_lmnl, batch_size=250, shuffle=True)\n",
    "\n",
    "# Crear un DataLoader para el conjunto de test\n",
    "dataset_test_lmnl = TensorDataset(x_mnl_test_tensor, x_mlp_test_tensor, y_test_dummy_tensor)\n",
    "test_loader_lmnl = DataLoader(dataset_test_lmnl, batch_size=len(x_test_tensor), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iii. Definir funciones`<br>\n",
    "No podemos reutilizar la función de evaluación que creamos antes, porque este modelo toma dos entradas (x_mnl, x_mlp), en lugar de una. Por lo tanto, creamos una nueva para el modelo L-MNL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lmnl(model, criterion, data_loader):\n",
    "    model.eval()  \n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_mnl, x_mlp, labels in data_loader:\n",
    "            outputs = model(x_mnl,x_mlp)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`iv. Create the L-MNL model object`<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions\n",
    "input_size_mnl = x_mnl_train_tensor.size()[1]  \n",
    "input_size_mlp = x_mlp_train_tensor.size()[1]  \n",
    "hidden_size1 = 10  \n",
    "hidden_size2 = 10  \n",
    "output_size = 3    \n",
    "print(f' input_size_mnl = {input_size_mnl}, input_size_mlp = {input_size_mlp}, hidden_size1 = {hidden_size1}, hidden_size2 = {hidden_size2}, output_size = {output_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invocar al modelo L-MNL\n",
    "model = LMNL(input_size_mlp, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "print_model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`v. Entrenar el modelo L-MNL`<br>\n",
    "Finalmente, ¡estamos listos para entrenar el modelo L-MNL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nEpoch = 1000  \n",
    "lr = 0.0001  \n",
    "status = 10  \n",
    "patience = 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función de pérdida y el optimizador\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr,  betas=(0.9, 0.999), eps=1e-07, weight_decay=0)\n",
    "\n",
    "# Definir las listas para almacenar las pérdidas\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Inicializar contador de parada anticipada \n",
    "counter = 0\n",
    "best_loss = np.inf  # Establecer la pérdida inicial a infinito\n",
    "\n",
    "# Bucle de entrenamiento\n",
    "for epoch in range(nEpoch):\n",
    "    running_loss = 0.0\n",
    "    for x_mnl, x_mlp, labels in train_loader_lmnl:\n",
    "        \n",
    "        # Poner a cero los gradientes\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x_mnl,x_mlp)   \n",
    "\n",
    "        # Calcular la perdida\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass y optimizacion\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() # Log-verosimilitud en el conjunto de entrenamiento\n",
    "\n",
    "    # Evaluar en el conjunto de prueba\n",
    "    test_loss = evaluate_lmnl(model, loss_fn, test_loader_lmnl)\n",
    "\n",
    "    if epoch == 0 or (epoch + 1) % status == 0:\n",
    "        print(f'Epoch [{epoch+1:5.0f}/{nEpoch}], Train Loss: {running_loss:0.3f}, Test Loss: {test_loss:0.3f}')\n",
    "\n",
    "    train_losses.append(running_loss)\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    # Implementar la parada anticipada\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f'Parada anticipada en la época {epoch+1}')\n",
    "            break\n",
    "        \n",
    "print(f'\\nEntrenamiento terminado.\\tPérdida de entrenamiento: {running_loss:0.3f}, Pérdida de test: {test_loss:0.3f}')\n",
    "show_loss_plot(train_losses, test_losses, num_obs_train, num_obs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la disposición a pagar para reducir el tiempo caminado al supermercado en un minuto\n",
    "LL_LMNL = best_loss\n",
    "B_transport_LMNL = model.B_transport.weight.data.item()\n",
    "B_stores_LMNL = model.B_stores.weight.data.item()\n",
    "\n",
    "WTP_LMNL = B_stores_LMNL/B_transport_LMNL\n",
    "print(f'\\nLa disposición a pagar para reducir un minuto el tiempo de caminata hasta el supermercado es: {WTP_LMNL:.2f} minutos caminando hasta el transporte público')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Reflexiones`\n",
    "1. El modelo L-MNL consigue un ajuste del modelo muy próximo al modelo MLP totalmente flexible\n",
    "1. El modelo L-MNL permite comprender la relación entre la distancia a los supermercados y la distancia al transporte público.\n",
    "1. Los resultados recuperan que la WTP no es estable, sino que fluctúa a lo largo de los entrenamientos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Ejercicio 4: Características L-MLP`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`A` Pruebe a reducir el número de características que se introducen en el MLP y evalúe el impacto en el rendimiento del modelo y la WTP. Es decir, utilice sólo «RESPCITY» y «WOMAN»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{green}{\\text{Añada aquí sus respuestas}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añade tu código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`A.ii` Volver a entrenar el modelo MLP utilizando la celda anterior:  <br>\n",
    "\n",
    "Pérdida de entrenamiento: 6019,589, Pérdida de test: 1378,459 y la Disposición a pagar por reducir un minuto el tiempo de caminata hasta los supermercados es: 1,02 minutos de caminata hasta el transporte público."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Ejercicio 5: Predicción utilizando L-MNL`<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un investigador desearía utilizar este modelo para hacer predicciones sobre la cuota de mercado. Sin embargo, esto es más difícil.<br>\n",
    "\n",
    "`A` Explique 2 razones por las que realizar previsiones con el modelo L-MNL es menos sencillo que con los modelos MNL.<br>\n",
    "`B` Explique 2 argumentos a favor de la utilización del modelo L-MNL como base para la elaboración de políticas.<br>\n",
    "`C` Supongamos que le contratan para ayudar y asesorar a este investigador. ¿Qué le aconsejaría, por ejemplo, en cuanto a la especificación del modelo L-MNL, la formación, la evaluación comparativa, etc.?<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{green}{\\text{Añada aquí sus respuestas}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añade tu código aquí"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
